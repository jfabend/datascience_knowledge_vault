The gradient boosting method assumes a real-valued y and seeks an approximation  F(x) in the form of a weighted sum of functions h_{i}(x)  from i learners

tries to find an approximation function
that minimizes the average value of the loss function on the training set, i.e., minimizes the empirical risk

It does so by starting with a model, consisting of a constant function and incrementally expands it in a [[greedy]] fashion

