Wie sollen bei der Backpropagation die Weights und Biases angepasst werden?

Folgende Optimizers gibt es:
[[Gradient Descent]]
[[Stochastic Gradient Descent]]
[[Adagrad]]
[[RMSprop]]
[[Adaprop]]
[[ADAM]]

Wenn man lokale Minima Ã¼berspringen will um das globale Minimum zu finden, kann man dies mit [[Momentum]] tun.

[[Optimizers PyTorch]]