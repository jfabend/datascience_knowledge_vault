Der Gradient Descent versucht die [[Loss Function Cost Function]] zu optimieren

Das Minimum in der Kostenfunktion finden => Den Hügel runterlaufen

Änderungsrate / Ableitung für den nächsten Iterationsschritt berechnen => In welche Richtung wird die Cost Function am meisten reduziert

Die Änderungsrate / Ableitung gibt grundsätzlich an, um welchen Teil sich der Funktionswert sich ändert pro bestimmten x-Intervall
dy/dx

Das Alpha im Gradient Descent ist die [[Learning Rate]]

