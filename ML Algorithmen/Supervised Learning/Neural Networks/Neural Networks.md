Preprocessing
- Categorical Features
	=> One Hot Encoding
	=> Ordinal Encoding
- [[(Image) Normalization]]
- Numerical Features
	=> [[Scaling]]

Basics und Bestandteile
[[Activation Functions]]
[[Loss Function Cost Function]]
[[Optimizers]]

Typically, we train any network for a number of epochs or cycles through the training dataset. Here are the steps that a training function performs as it iterates over the training dataset:

1.  Prepares all input images and label data for training
2.  Passes the input through the network (forward pass)
3.  Computes the loss (how far is the predicted classes are from the correct labels)
4.  Propagates gradients back into the networkâ€™s parameters (backward pass)
5.  Updates the weights (parameter update)

It repeats this process until the average loss has sufficiently decreased.


[[NN Parameters]]


Anwendungsgebiete
[[NLP or Text]]

Klassen und Typen:
[[Adaptive Resonanztheorie]]
[[Boltzmann-Maschine]]
[[Cascade-Correlation-Netze]]
[[Counterpropagation-Netze]]
[[Growing Neural Gas]]
[[Lernende Vektorquantisierung]]
[[Lernmatrix]]
[[McCulloch-Pitts-Netze]]
[[Neocognitron]]
[[Oszillierendes neuronales Netz]]
[[Perzeptron]]
[[Probabilistische neuronale Netze]]
[[Radiale neuronale Netze]]
[[Rekurrente neuronale Netze]]
[[Residuale neuronale Netze]]
[[Self-Organizing Maps]]
[[Spiking Neural Networks]]
[[Time Delay Neural Networks]]
